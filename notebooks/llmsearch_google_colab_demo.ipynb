{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNDpiAzBI0xH"
      },
      "source": [
        "# LLMSearch Google Colab Demo\n",
        "\n",
        "This notebook was tested to run the following 13B model - https://huggingface.co/TheBloke/airoboros-l2-13B-gpt4-1.4.1-GGUF\n",
        "\n",
        "In case of memory errors, tweak the config to offload some layers to CPU, or try a smaller model.\n",
        "\n",
        "## Instuctions\n",
        "\n",
        "* Upload or generate some documents (check supported format in README.md) in `sample_docs` folder.\n",
        "    * Or use a sample pdf book provided - Pro Git - https://git-scm.com/book/en/v2\n",
        "* Run the notebook.\n",
        "* Optional - tweak configuration file to point to a different model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43kUuvFzDxFr"
      },
      "source": [
        "### Prepare configuration and download the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2s1HFkwBHNe",
        "outputId": "f6214a2a-2764-4a63-86da-21f06b82cb88"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "\n",
        "# Make folder structure\n",
        "mkdir -p llm/embeddings llm/cache llm/models llm/config sample_docs\n",
        "\n",
        "# Download sample book\n",
        "wget -P sample_docs https://github.com/progit/progit2/releases/download/2.1.413/progit.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%shell\n",
        "\n",
        "# Generate sample configuration\n",
        "\n",
        "cat << EOF > llm/config/config.yaml\n",
        "\n",
        "cache_folder: /content/llm/cache\n",
        "\n",
        "embeddings:\n",
        "  embeddings_path: /content/llm/embeddings\n",
        "  chunk_sizes:\n",
        "    - 1024\n",
        "  document_settings:\n",
        "  - doc_path: sample_docs/\n",
        "    scan_extensions:\n",
        "      - md\n",
        "      - pdf\n",
        "    additional_parser_settings:\n",
        "      md:\n",
        "        skip_first: True\n",
        "        merge_sections: True\n",
        "        remove_images: True\n",
        "\n",
        "semantic_search:\n",
        "  search_type: similarity # mmr\n",
        "  max_char_size: 3096\n",
        "\n",
        "  reranker:\n",
        "    enabled: True\n",
        "    model: \"marco\" # for `BAAI/bge-reranker-base` or \"marco\" for cross-encoder/ms-marco-MiniLM-L-6-v2\n",
        "EOF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%shell\n",
        "\n",
        "\n",
        "cat << EOF > llm/config/model.yaml\n",
        "# Geberate sample model configuration for llama-cpp\n",
        "llm:\n",
        " type: llamacpp\n",
        " params:\n",
        "   model_path: /content/llm/models/airoboros-l2-13b-gpt4-1.4.1.Q4_K_M.gguf\n",
        "   prompt_template: |\n",
        "         ### Instruction:\n",
        "         Use the following pieces of context to provide detailed answer the question at the end. If answer isn't in the context, say that you don't know, don't try to make up an answer.\n",
        "\n",
        "         ### Context:\n",
        "         ---------------\n",
        "         {context}\n",
        "         ---------------\n",
        "\n",
        "         ### Question: {question}\n",
        "         ### Response:\n",
        "   model_init_params:\n",
        "     n_ctx: 1024\n",
        "     n_batch: 512\n",
        "     n_gpu_layers: 43\n",
        "\n",
        "   model_kwargs:\n",
        "     max_tokens: 512\n",
        "     top_p: 0.1\n",
        "     top_k: 40\n",
        "     temperature: 0.2\n",
        "\n",
        "EOF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmnHPhTNEWub",
        "outputId": "5d2e5ab2-daf9-4c05-8d48-5bc68636ba0c"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "\n",
        "# Download the model\n",
        "# Sample model - https://huggingface.co/TheBloke/WizardLM-13B-Uncensored-GGML/tree/main\n",
        "# Optionally download a smaller model to test...\n",
        "\n",
        "\n",
        "cd llm/models\n",
        "wget https://huggingface.co/TheBloke/airoboros-l2-13B-gpt4-1.4.1-GGUF/resolve/main/airoboros-l2-13b-gpt4-1.4.1.Q4_K_M.gguf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0UkSdWrAByA"
      },
      "source": [
        "### Enable building with CUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fESaAtai3BFH",
        "outputId": "a45e8a0a-43da-40a3-ea2c-6b093fd7d449"
      },
      "outputs": [],
      "source": [
        "%env CMAKE_ARGS=-DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc -DLLAMA_CUBLAS=ON\n",
        "%env FORCE_CMAKE=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zs6ZdRgO7Kxz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Install torch and torchvision\n",
        "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "daEnZje03nHX",
        "outputId": "7ec01beb-d7f1-4b20-cf7d-62dfaf4ecf6f"
      },
      "outputs": [],
      "source": [
        "!pip install --no-cache-dir git+https://github.com/tghattas/llm-search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Qjl2MxoA4uc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaOl4lF-4uEE",
        "outputId": "84e383d0-04ac-4c87-c33e-480fede603d7"
      },
      "outputs": [],
      "source": [
        "! llmsearch index create -c llm/config/config.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOU3Fksc_vr9",
        "outputId": "05d0fe9f-b9ff-431c-e2dd-e403db6d9776"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "\n",
        "llmsearch interact llm -c llm/config/config.yaml -m llm/config/model.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
