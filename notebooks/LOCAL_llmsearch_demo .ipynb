{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNDpiAzBI0xH"
      },
      "source": [
        "# LLMSearch Local Macbook (with M chip) \n",
        "\n",
        "In case of memory errors, tweak the config to offload some layers to CPU, or try a smaller model.\n",
        "\n",
        "## Instuctions\n",
        "\n",
        "* Upload or generate some documents (check supported format in README.md) in `sample_docs` folder.\n",
        "    * Or use a sample pdf book provided - Pro Git - https://git-scm.com/book/en/v2\n",
        "* Run the notebook.\n",
        "* Optional - tweak configuration file to point to a different model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43kUuvFzDxFr"
      },
      "source": [
        "### Prepare configuration and download the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2s1HFkwBHNe",
        "outputId": "f6214a2a-2764-4a63-86da-21f06b82cb88"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# Make folder structure\n",
        "mkdir -p llm/embeddings llm/cache llm/models llm/config sample_docs\n",
        "\n",
        "# Download sample book\n",
        "wget -P sample_docs https://github.com/progit/progit2/releases/download/2.1.413/progit.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# Generate sample configuration\n",
        "\n",
        "cat << EOF > llm/config/config.yaml\n",
        "\n",
        "cache_folder: ./llm/cache\n",
        "\n",
        "embeddings:\n",
        "  embeddings_path: ./llm/embeddings\n",
        "  chunk_sizes:\n",
        "    - 1024\n",
        "  document_settings:\n",
        "  - doc_path: sample_docs/\n",
        "    scan_extensions:\n",
        "      - md\n",
        "      - pdf\n",
        "    additional_parser_settings:\n",
        "      md:\n",
        "        skip_first: True\n",
        "        merge_sections: True\n",
        "        remove_images: True\n",
        "\n",
        "semantic_search:\n",
        "  search_type: similarity # mmr\n",
        "  max_char_size: 3096\n",
        "\n",
        "  reranker:\n",
        "    enabled: True\n",
        "    model: \"marco\" # for `BAAI/bge-reranker-base` or \"marco\" for cross-encoder/ms-marco-MiniLM-L-6-v2\n",
        "EOF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "\n",
        "cat << EOF > llm/config/model.yaml\n",
        "# Geberate sample model configuration for llama-cpp\n",
        "llm:\n",
        " type: llamacpp\n",
        " params:\n",
        "   model_path: ./llm/models/airoboros-l2-13b-gpt4-1.4.1.Q4_K_M.gguf\n",
        "   prompt_template: |\n",
        "         ### Instruction:\n",
        "         Use the following pieces of context to provide detailed answer the question at the end. If answer isn't in the context, say that you don't know, don't try to make up an answer.\n",
        "\n",
        "         ### Context:\n",
        "         ---------------\n",
        "         {context}\n",
        "         ---------------\n",
        "\n",
        "         ### Question: {question}\n",
        "         ### Response:\n",
        "   model_init_params:\n",
        "     n_ctx: 1024\n",
        "     n_batch: 512\n",
        "     n_gpu_layers: 43\n",
        "\n",
        "   model_kwargs:\n",
        "     max_tokens: 512\n",
        "     top_p: 0.1\n",
        "     top_k: 40\n",
        "     temperature: 0.2\n",
        "\n",
        "EOF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# Download the model\n",
        "# Sample model - https://huggingface.co/TheBloke/WizardLM-13B-Uncensored-GGML/tree/main\n",
        "# Optionally download a smaller model to test...\n",
        "\n",
        "\n",
        "cd llm/models\n",
        "wget https://huggingface.co/TheBloke/airoboros-l2-13B-gpt4-1.4.1-GGUF/resolve/main/airoboros-l2-13b-gpt4-1.4.1.Q4_K_M.gguf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Install torch and torchvision\n",
        "%pip install torch torchvision "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "daEnZje03nHX",
        "outputId": "7ec01beb-d7f1-4b20-cf7d-62dfaf4ecf6f"
      },
      "outputs": [],
      "source": [
        "%pip install torch torchvision \n",
        "%pip install --no-cache-dir git+https://github.com/tghattas/llm-search\n",
        "%pip install -U sqlalchemy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaOl4lF-4uEE",
        "outputId": "84e383d0-04ac-4c87-c33e-480fede603d7"
      },
      "outputs": [],
      "source": [
        "!llmsearch index create -c llm/config/config.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOU3Fksc_vr9",
        "outputId": "05d0fe9f-b9ff-431c-e2dd-e403db6d9776"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "llmsearch interact llm -c llm/config/config.yaml -m llm/config/model.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_cpp import Llama\n",
        "model = Llama(model_path=\"./llm/models/airoboros-l2-13b-gpt4-1.4.1.Q4_K_M.gguf\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model(\n",
        "      \"Q: Name the planets in the solar system? A: \", # Prompt\n",
        "      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
        "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
        "      echo=True # Echo the prompt back in the output\n",
        "))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
