{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNDpiAzBI0xH"
      },
      "source": [
        "# LLMSearch Local Macbook (with M chip) \n",
        "\n",
        "In case of memory errors, tweak the config to offload some layers to CPU, or try a smaller model.\n",
        "\n",
        "## Instuctions\n",
        "\n",
        "* Upload or generate some documents (check supported format in README.md) in `sample_docs` folder.\n",
        "    * Or use a sample pdf book provided - Pro Git - https://git-scm.com/book/en/v2\n",
        "* Run the notebook.\n",
        "* Optional - tweak configuration file to point to a different model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43kUuvFzDxFr"
      },
      "source": [
        "### Prepare configuration and download the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2s1HFkwBHNe",
        "outputId": "f6214a2a-2764-4a63-86da-21f06b82cb88"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# Make folder structure\n",
        "mkdir -p llm/embeddings llm/cache llm/models llm/config sample_docs\n",
        "\n",
        "# Download sample book\n",
        "wget -P sample_docs https://github.com/progit/progit2/releases/download/2.1.413/progit.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# Generate sample configuration\n",
        "\n",
        "cat << EOF > llm/config/config.yaml\n",
        "\n",
        "cache_folder: ./llm/cache\n",
        "\n",
        "embeddings:\n",
        "  embeddings_path: ./llm/embeddings\n",
        "  chunk_sizes:\n",
        "    - 1024\n",
        "  document_settings:\n",
        "  - doc_path: sample_docs/\n",
        "    scan_extensions:\n",
        "      - md\n",
        "      - pdf\n",
        "    additional_parser_settings:\n",
        "      md:\n",
        "        skip_first: True\n",
        "        merge_sections: True\n",
        "        remove_images: True\n",
        "\n",
        "semantic_search:\n",
        "  search_type: similarity # mmr\n",
        "  max_char_size: 3096\n",
        "\n",
        "  reranker:\n",
        "    enabled: True\n",
        "    model: \"marco\" # for `BAAI/bge-reranker-base` or \"marco\" for cross-encoder/ms-marco-MiniLM-L-6-v2\n",
        "EOF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "\n",
        "cat << EOF > llm/config/model.yaml\n",
        "# Geberate sample model configuration for llama-cpp\n",
        "llm:\n",
        " type: llamacpp\n",
        " params:\n",
        "   model_path: ./llm/models/airoboros-l2-13b-gpt4-1.4.1.Q4_K_M.gguf\n",
        "   prompt_template: |\n",
        "         ### Instruction:\n",
        "         Use the following pieces of context to provide detailed answer the question at the end. If answer isn't in the context, say that you don't know, don't try to make up an answer.\n",
        "\n",
        "         ### Context:\n",
        "         ---------------\n",
        "         {context}\n",
        "         ---------------\n",
        "\n",
        "         ### Question: {question}\n",
        "         ### Response:\n",
        "   model_init_params:\n",
        "     n_ctx: 1024\n",
        "     n_batch: 512\n",
        "     n_gpu_layers: 43\n",
        "\n",
        "   model_kwargs:\n",
        "     max_tokens: 512\n",
        "     top_p: 0.1\n",
        "     top_k: 40\n",
        "     temperature: 0.2\n",
        "\n",
        "EOF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# Download the model\n",
        "# Sample model - https://huggingface.co/TheBloke/WizardLM-13B-Uncensored-GGML/tree/main\n",
        "# Optionally download a smaller model to test...\n",
        "\n",
        "\n",
        "cd llm/models\n",
        "wget https://huggingface.co/TheBloke/airoboros-l2-13B-gpt4-1.4.1-GGUF/resolve/main/airoboros-l2-13b-gpt4-1.4.1.Q4_K_M.gguf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Install torch and torchvision\n",
        "%pip install torch torchvision #--index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "daEnZje03nHX",
        "outputId": "7ec01beb-d7f1-4b20-cf7d-62dfaf4ecf6f"
      },
      "outputs": [],
      "source": [
        "%pip install --no-cache-dir git+https://github.com/tghattas/llm-search\n",
        "%pip install -U sqlalchemy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaOl4lF-4uEE",
        "outputId": "84e383d0-04ac-4c87-c33e-480fede603d7"
      },
      "outputs": [],
      "source": [
        "!llmsearch index create -c llm/config/config.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOU3Fksc_vr9",
        "outputId": "05d0fe9f-b9ff-431c-e2dd-e403db6d9776"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/tamer/PycharmProjects/llm-search/venv/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "2024-02-24 11:26:28.655 | INFO     | llmsearch.config:load_yaml_file:233 - Loading doc config from a file: llm/config/config.yaml\n",
            "2024-02-24 11:26:28.656 | INFO     | llmsearch.config:load_yaml_file:233 - Loading doc config from a file: llm/config/model.yaml\n",
            "2024-02-24 11:26:28.657 | INFO     | llmsearch.config:validate_params:175 - Loading model paramaters in configuration class LlamaModelConfig\n",
            "2024-02-24 11:26:28.657 | INFO     | llmsearch.utils:set_cache_folder:43 - Setting SENTENCE_TRANSFORMERS_HOME folder: llm/cache\n",
            "2024-02-24 11:26:28.657 | INFO     | llmsearch.utils:set_cache_folder:46 - Setting TRANSFORMERS_CACHE folder: llm/cache/transformers\n",
            "2024-02-24 11:26:28.657 | INFO     | llmsearch.utils:set_cache_folder:47 - Setting HF_HOME: llm/cache/hf_home\n",
            "2024-02-24 11:26:28.657 | INFO     | llmsearch.utils:set_cache_folder:48 - Setting MODELS_CACHE_FOLDER: llm/cache\n",
            "2024-02-24 11:26:28.658 | INFO     | llmsearch.models.llama:model:139 - Loading model...\n",
            "2024-02-24 11:26:28.658 | INFO     | llmsearch.models.llama:model:142 - Initializing LLAmaCPP model...\n",
            "2024-02-24 11:26:28.658 | INFO     | llmsearch.models.llama:model:143 - {'n_ctx': 1024, 'n_batch': 512, 'n_gpu_layers': 43}\n",
            "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from ./llm/models/airoboros-l2-13b-gpt4-1.4.1.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   81 tensors\n",
            "llama_model_loader: - type q4_K:  241 tensors\n",
            "llama_model_loader: - type q6_K:   41 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 5120\n",
            "llm_load_print_meta: n_head           = 40\n",
            "llm_load_print_meta: n_head_kv        = 40\n",
            "llm_load_print_meta: n_layer          = 40\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
            "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 13824\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 13B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 13.02 B\n",
            "llm_load_print_meta: model size       = 7.33 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.28 MiB\n",
            "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  7412.97 MiB, ( 7413.03 / 21845.34)\n",
            "llm_load_tensors: offloading 40 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 41/41 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =    87.89 MiB\n",
            "llm_load_tensors:      Metal buffer size =  7412.97 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 1024\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "ggml_metal_init: allocating\n",
            "ggml_metal_init: found device: Apple M1 Pro\n",
            "ggml_metal_init: picking default device: Apple M1 Pro\n",
            "ggml_metal_init: default.metallib not found, loading from source\n",
            "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
            "ggml_metal_init: loading '/Users/tamer/PycharmProjects/llm-search/venv/lib/python3.10/site-packages/llama_cpp/ggml-metal.metal'\n",
            "ggml_metal_init: GPU name:   Apple M1 Pro\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
            "ggml_metal_init: simdgroup reduction support   = true\n",
            "ggml_metal_init: simdgroup matrix mul. support = true\n",
            "ggml_metal_init: hasUnifiedMemory              = true\n",
            "ggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB\n",
            "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   800.00 MiB, ( 8214.59 / 21845.34)\n",
            "llama_kv_cache_init:      Metal KV buffer size =   800.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  800.00 MiB, K (f16):  400.00 MiB, V (f16):  400.00 MiB\n",
            "llama_new_context_with_model:        CPU input buffer size   =    13.01 MiB\n",
            "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   122.02 MiB, ( 8336.61 / 21845.34)\n",
            "llama_new_context_with_model:      Metal compute buffer size =   122.00 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    10.00 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 3\n",
            "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '40', 'llama.context_length': '4096', 'llama.attention.head_count': '40', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '13824', 'llama.embedding_length': '5120', 'llama.block_count': '40', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'LLaMA v2'}\n",
            "2024-02-24 11:26:28.853 | INFO     | llmsearch.embeddings:get_embedding_model:67 - Embedding model config: type=<EmbeddingModelType.instruct: 'instruct'> model_name='hkunlp/instructor-large' additional_kwargs={}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load INSTRUCTOR_Transformer\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/tamer/PycharmProjects/llm-search/venv/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/Users/tamer/PycharmProjects/llm-search/venv/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max_seq_length  512\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-24 11:26:31.295 | INFO     | llmsearch.ranking:__init__:20 - Initializing Reranker...\n",
            "2024-02-24 11:26:32.280 | INFO     | llmsearch.ranking:__init__:23 - Initialized MS-MARCO Reranker\n",
            "2024-02-24 11:26:32.280 | INFO     | llmsearch.splade:__init__:37 - Setting device to cpu\n",
            "2024-02-24 11:26:33.018 | INFO     | llmsearch.splade:load:113 - SPLADE: Got 0 labels.\n",
            "2024-02-24 11:26:33.018 | INFO     | llmsearch.splade:load:119 - Loaded sparse (SPLADE) embeddings from ./llm/embeddings/splade/splade_embeddings.npz\n",
            "2024-02-24 11:26:33.018 | INFO     | llmsearch.utils:get_hyde_chain:116 - Creating HyDE chain...\n",
            "2024-02-24 11:26:33.018 | INFO     | llmsearch.utils:get_multiquery_chain:127 - Creating MultiQUery chain...\n",
            "\n",
            "Aborted!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-24 11:26:33.112 | WARNING  | llmsearch.models.llama:__del__:46 - Model does not have a __del__ method, so it may not be properly closed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ENTER QUESTION >> "
          ]
        },
        {
          "ename": "CalledProcessError",
          "evalue": "Command 'b'\\nllmsearch interact llm -c llm/config/config.yaml -m llm/config/model.yaml\\n'' returned non-zero exit status 1.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mllmsearch interact llm -c llm/config/config.yaml -m llm/config/model.yaml\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/PycharmProjects/llm-search/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2517\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2516\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2517\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
            "File \u001b[0;32m~/PycharmProjects/llm-search/venv/lib/python3.10/site-packages/IPython/core/magics/script.py:154\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/PycharmProjects/llm-search/venv/lib/python3.10/site-packages/IPython/core/magics/script.py:314\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\nllmsearch interact llm -c llm/config/config.yaml -m llm/config/model.yaml\\n'' returned non-zero exit status 1."
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "llmsearch interact llm -c llm/config/config.yaml -m llm/config/model.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_cpp import Llama\n",
        "model = Llama(model_path=\"./llm/models/airoboros-l2-13b-gpt4-1.4.1.Q4_K_M.gguf\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model(\n",
        "      \"Q: Name the planets in the solar system? A: \", # Prompt\n",
        "      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
        "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
        "      echo=True # Echo the prompt back in the output\n",
        "))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
